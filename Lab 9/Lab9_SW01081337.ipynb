{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4855b7",
   "metadata": {},
   "source": [
    "# CISB5123 Text Analytics\n",
    "## Lab 9.1 - Topic Modelling\n",
    "\n",
    "Name: Abdul Hakiim bin Ahmad Rosli (SW01081337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc242a5",
   "metadata": {},
   "source": [
    "Topic modeling is a type of statistical modeling used to identify topics or themes within a collection of documents. It involves automatically clustering words that tend to co-occur frequently across multiple documents, with the aim of identifying groups of words that represent distinct topics. The ultimate goal is to identify the underlying themes or topics that run through a large corpus of text data.\n",
    "\n",
    "In this lab, you will learn how to perform topic modeling using Gensim, a popular Python library for topic modeling, and the Latent Dirichlet Allocation (LDA) algorithm to discover abstract topics within a collection of documents.\n",
    "\n",
    "The general process of topic modeling includes:\n",
    "- Import the necessary libraries (i.e. NLTK, Gensim, Scikit-learn)\n",
    "- Load data\n",
    "- Preprocess the data\n",
    "- Create a document-term matrix\n",
    "- Run topic modeling algorithm\n",
    "- Interpret the results\n",
    "- Refine the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284e14b",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418bf33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text processing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modelling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download nltk resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dc7f4",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5778ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "    \"Rafael Nadal Is Out of the Australian Open\",\n",
    "    \"Biden Announces Virus Measures\",\n",
    "    \"Biden's Virus Plans Meet Reality\",\n",
    "    \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39903db3",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfa74a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rafael', 'nadal', 'join', 'roger', 'federer', 'missing', 'open'],\n",
       " ['rafael', 'nadal', 'australian', 'open'],\n",
       " ['biden', 'announces', 'virus', 'measure'],\n",
       " ['biden', 'virus', 'plan', 'meet', 'reality'],\n",
       " ['biden', 'virus', 'plan', 'stand']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize a WordNet lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize the text and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Filter out alpha-numeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each token\n",
    "    return tokens  # Return the preprocessed tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]  # Preprocess each document in the list\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8bf26",
   "metadata": {},
   "source": [
    "#### Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07942fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a05043",
   "metadata": {},
   "source": [
    "#### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3ee114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus: bag-of-words representation of the documents\n",
    "# num_topics: number of topics to be extracted by the model\n",
    "# id2word=dictionary: dictionary mapping from word IDs to words\n",
    "# passes: number of passes through the corpus during training\n",
    "# train an LDA model on the corpus with 4 topics using Gensim's LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a3ea5",
   "metadata": {},
   "source": [
    "#### Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399ae21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic: \n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      0\n",
      "1         Rafael Nadal Is Out of the Australian Open      0\n",
      "2                     Biden Announces Virus Measures      1\n",
      "3                   Biden's Virus Plans Meet Reality      1\n",
      "4                    Where Biden's Virus Plan Stands      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# Iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to box representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append to the list\n",
    "    article_labels.append(dominant_topic)\n",
    "    \n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "    \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic: \")\n",
    "print(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b0cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for each topic:\n",
      "Topic 0\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"federer\" (weight: 0.079)\n",
      "- \"missing\" (weight: 0.079)\n",
      "- \"roger\" (weight: 0.079)\n",
      "- \"join\" (weight: 0.079)\n",
      "- \"australian\" (weight: 0.079)\n",
      "- \"virus\" (weight: 0.027)\n",
      "- \"biden\" (weight: 0.027)\n",
      "\n",
      "Topic 1\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.119)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"australian\" (weight: 0.024)\n",
      "- \"open\" (weight: 0.024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top term of each topic\n",
    "print(\"Top Terms for each topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2af073",
   "metadata": {},
   "source": [
    "Topic 1 seems to be related around politics and virus, where the weight of terms like\n",
    "\"biden\" and \"virus\" are particularly high, indicating their significance in this topic.\n",
    "\n",
    "Topic 0 seems to be related to tennis, where the weight of terms like \"nadal\" and \"rafael\"\n",
    "are relatively high, suggesting a strong association with this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b372466",
   "metadata": {},
   "source": [
    "### Using articles (‘npr.csv’) from NPR (National Public Radio) obtained from their website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535f957",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d473deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d7a1e",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6e0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('npr.csv')\n",
    "documents = df['Article'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016990e",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eafd44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', '2016', 'even', 'policy', 'bipartisan', 'politics', 'sense', 'year', 'show', 'little', 'sign', 'ending', 'president', 'obama', 'moved', 'sanction', 'russia', 'alleged', 'interference', 'election', 'concluded', 'republican', 'long', 'called', 'similar', 'severe', 'measure', 'could', 'scarcely', 'bring', 'approve', 'house', 'speaker', 'paul', 'ryan', 'called', 'obama', 'measure', 'appropriate', 'also', 'overdue', 'prime', 'example', 'administration', 'ineffective', 'foreign', 'policy', 'left', 'america', 'weaker', 'eye', 'gop', 'leader', 'sounded', 'much', 'theme', 'urging', 'president', 'obama', 'year', 'take', 'strong', 'action', 'deter', 'russia', 'worldwide', 'aggression', 'including', 'operation', 'wrote', 'devin', 'nunes', 'chairman', 'house', 'intelligence', 'committee', 'week', 'left', 'office', 'president', 'suddenly', 'decided', 'stronger', 'measure', 'indeed', 'appearing', 'cnn', 'frequent', 'obama', 'critic', 'trent', 'frank', 'called', 'much', 'tougher', 'action', 'said', 'three', 'time', 'obama', 'finally', 'found', 'meanwhile', 'fox', 'news', 'various', 'spokesman', 'trump', 'said', 'obama', 'real', 'target', 'russian', 'man', 'poised', 'take', 'white', 'house', 'le', 'three', 'week', 'spoke', 'obama', 'trying', 'tie', 'trump', 'hand', 'box', 'meaning', 'would', 'forced', 'either', 'keep', 'sanction', 'odds', 'republican', 'want', 'tougher', 'still', 'moscow', 'throughout', '2016', 'trump', 'repeatedly', 'called', 'sanction', 'closer', 'tie', 'russia', 'including', 'cooperation', 'fight', 'isi', 'russia', 'battled', 'isi', 'syria', 'behalf', 'country', 'embattled', 'dictator', 'bashar', 'assad', 'bombing', 'besieged', 'city', 'aleppo', 'fell', 'assad', 'force', 'week', 'campaign', 'trump', 'even', 'urged', 'russia', 'find', 'missing', 'email', 'private', 'server', 'opponent', 'hillary', 'clinton', 'exchanged', 'public', 'encomium', 'russian', 'president', 'vladimir', 'putin', 'several', 'occasion', 'added', 'doubt', 'current', 'level', 'support', 'nato', 'putin', 'longtime', 'nemesis', 'also', 'suggestion', 'trump', 'extensive', 'business', 'dealing', 'various', 'russian', 'reason', 'refuse', 'release', 'tax', 'return', 'issue', 'disquieting', 'republican', 'many', 'month', 'sen', 'john', 'mccain', 'lindsay', 'graham', 'prominent', 'senior', 'member', 'armed', 'service', 'committee', 'accepted', 'assessment', '17', 'intelligence', 'agency', 'regarding', 'role', 'russia', 'hacking', 'various', 'democratic', 'committee', 'last', 'year', 'includes', 'fbi', 'cia', 'consensus', 'russian', 'goal', 'discredit', 'american', 'democracy', 'defeat', 'clinton', 'elect', 'trump', 'say', 'great', 'majority', 'senate', 'colleague', 'agree', 'mccain', 'slated', 'armed', 'service', 'hearing', 'cyberthreats', 'politicizing', 'russian', 'action', 'idea', 'helped', 'trump', 'win', 'also', 'made', 'issue', 'difficult', 'republican', 'leader', 'allowed', 'trump', 'supporter', 'push', 'back', 'intelligence', 'agency', 'say', 'entire', 'issue', 'designed', 'undermine', 'trump', 'legitimacy', 'senate', 'majority', 'leader', 'mitch', 'mcconnell', 'far', 'resisted', 'call', 'select', 'committee', 'look', 'russian', 'interference', '2016', 'campaign', 'said', 'enough', 'richard', 'burr', 'look', 'chairman', 'senate', 'intelligence', 'committee', 'typically', 'republican', 'leader', 'spokesman', 'say', 'evidence', 'actual', 'voting', 'tallying', '8', 'compromised', 'true', 'also', 'red', 'herring', 'interference', 'function', 'alleged', 'focus', 'intelligence', 'agency', 'concern', 'part', 'trump', 'shown', 'little', 'interest', 'delving', 'happened', 'cast', 'doubt', 'intelligence', 'report', 'date', 'suggested', 'one', 'really', 'know', 'also', 'suggested', 'computer', 'make', 'difficult', 'know', 'using', 'week', 'trump', 'said', 'time', 'get', 'life', 'important', 'however', 'week', 'end', 'agree', 'intelligence', 'briefing', 'subject', 'next', 'week', 'wanted', 'daily', 'intelligence', 'briefing', 'available', 'recent', 'week', 'preferring', 'given', 'men', 'chosen', 'vice', 'president', 'mike', 'penny', 'national', 'security', 'adviser', 'mike', 'flynn', 'trump', 'taking', 'occasionally', 'irony', 'controversy', 'arising', 'eleventh', 'hour', 'obama', 'presidency', 'scarcely', 'overstated', 'defines', 'dilemma', 'facing', 'outgoing', 'president', 'incoming', 'party', 'control', 'obama', 'appears', 'reluctant', 'retaliate', 'russian', 'hacking', 'election', 'fear', 'seeming', 'interfere', 'election', 'republican', 'meanwhile', 'year', 'called', 'greater', 'confrontation', 'russian', 'obama', 'usually', 'resisting', 'obama', 'join', 'nato', 'punishing', 'russian', 'economic', 'sanction', 'annexation', 'crimea', 'sanction', 'may', 'painful', 'coming', 'alongside', 'falling', 'price', 'oil', 'commodity', 'keep', 'russian', 'economy', 'afloat', 'occasion', 'despite', 'russian', 'provocation', 'surrogate', 'syria', 'elsewhere', 'obama', 'make', 'overt', 'move', 'force', 'russia', 'hand', 'includes', 'occasion', 'russia', 'believed', 'hacking', 'critical', 'computer', 'system', 'neighboring', 'ukraine', 'estonia', 'poland', 'week', 'following', 'chorus', 'confirmation', 'intelligence', 'community', 'regarding', 'russian', 'role', 'computer', 'hacking', 'political', 'campaign', 'obama', 'acted', 'imposed', 'set', 'mostly', 'diplomatic', 'action', 'sanctioning', 'russian', 'official', 'closing', 'two', 'diplomatic', 'compound', 'expelling', '35', 'russian', 'diplomat', 'may', 'damaging', 'measure', 'taken', 'covertly', 'russophobes', 'washington', 'held', 'hope', 'visible', 'portion', 'program', 'scarcely', 'amounted', 'major', 'retribution', 'putin', 'saw', 'fit', 'diminish', 'obama', 'sanction', 'declining', 'respond', 'although', 'government', 'steadfastly', 'denied', 'interference', 'election', 'putin', 'rejected', 'foreign', 'minister', 'recommended', 'package', 'response', 'even', 'sent', 'invitation', 'diplomat', 'send', 'child', 'holiday', 'party', 'moscow', 'allowed', 'putin', 'appear', 'moment', 'bigger', 'man', 'even', 'spurned', 'obama', 'kept', 'looked', 'like', 'public', 'bromance', 'trump', 'tweeted', 'great', 'move', 'delay', 'putin', 'always', 'knew', 'smart', 'moment', 'may', 'seem', 'overall', 'russia', 'question', 'amount', 'first', 'crisis', 'facing', 'trump', 'presidency', 'whether', 'forced', 'campaign', 'interference', 'issue', 'trump', 'must', 'grasp', 'nettle', 'relationship', 'mitt', 'romney', 'called', 'greatest', 'threat', 'security', 'world', 'sure', 'trump', 'need', 'dispel', 'doubt', 'ability', 'stand', 'putin', 'bullied', 'cajoled', 'way', 'center', 'stage', 'recent', 'world', 'affair', 'trump', 'also', 'seems', 'determined', 'turn', 'page', 'past', 'commitment', 'free', 'trade', 'philosophy', 'funding', 'nato', 'united', 'nation', 'twitter', 'account', 'guide', 'trump', 'show', 'little', 'concern', 'conundrum', 'others', 'perceive', 'facing', 'trump', 'shown', 'determined', 'play', 'rule', 'year', 'ago', 'many', 'confident', 'would', 'work', 'world', 'presidential', 'politics', 'find', 'whether', 'work', 'oval', 'office']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialize a WordNet lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()] # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize each token\n",
    "    return tokens # Return the preprocessed tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents] # Preprocess each document in the list\n",
    "\n",
    "print(preprocessed_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8df70e",
   "metadata": {},
   "source": [
    "#### Create document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e33011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f6f4d",
   "metadata": {},
   "source": [
    "#### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a267098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15) \n",
    "# Train an LDA model on the corpus with 2 topics using Gensim's LdaModel class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43db06a",
   "metadata": {},
   "source": [
    "#### Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64693c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      In the Washington of 2016, even when the polic...      4\n",
      "1        Donald Trump has used Twitter  —   his prefe...      4\n",
      "2        Donald Trump is unabashedly praising Russian...      4\n",
      "3      Updated at 2:50 p. m. ET, Russian President Vl...      4\n",
      "4      From photography, illustration and video, to d...      0\n",
      "...                                                  ...    ...\n",
      "11987  The number of law enforcement officers shot an...      1\n",
      "11988    Trump is busy these days with victory tours,...      4\n",
      "11989  It’s always interesting for the Goats and Soda...      2\n",
      "11990  The election of Donald Trump was a surprise to...      4\n",
      "11991  Voters in the English city of Sunderland did s...      1\n",
      "\n",
      "[11992 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append to the list\n",
    "    article_labels.append(dominant_topic)\n",
    "    \n",
    "# Create DataFrame\n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfc2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['know', 'think', 'thing', 'life', 'woman', 'really', 'story', 'show', 'book', 'back']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['police', 'country', 'report', 'city', 'government', 'state', 'attack', 'told', 'war', 'two']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['food', 'study', 'water', 'disease', 'human', 'scientist', 'university', 'science', 'animal', 'research']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['health', 'school', 'percent', 'state', 'company', 'student', 'care', 'program', 'child', 'million']\n",
      "\n",
      "Top terms for Topic #4:\n",
      "['trump', 'clinton', 'president', 'state', 'republican', 'campaign', 'election', 'obama', 'vote', 'voter']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Top terms for Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1345f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"thing\" (weight: 0.005)\n",
      "- \"life\" (weight: 0.005)\n",
      "- \"woman\" (weight: 0.005)\n",
      "- \"really\" (weight: 0.004)\n",
      "- \"story\" (weight: 0.004)\n",
      "- \"show\" (weight: 0.003)\n",
      "- \"book\" (weight: 0.003)\n",
      "- \"back\" (weight: 0.003)\n",
      "\n",
      "Topic 1:\n",
      "- \"police\" (weight: 0.007)\n",
      "- \"country\" (weight: 0.006)\n",
      "- \"report\" (weight: 0.005)\n",
      "- \"city\" (weight: 0.005)\n",
      "- \"government\" (weight: 0.005)\n",
      "- \"state\" (weight: 0.004)\n",
      "- \"attack\" (weight: 0.004)\n",
      "- \"told\" (weight: 0.004)\n",
      "- \"war\" (weight: 0.003)\n",
      "- \"two\" (weight: 0.003)\n",
      "\n",
      "Topic 2:\n",
      "- \"food\" (weight: 0.007)\n",
      "- \"study\" (weight: 0.005)\n",
      "- \"water\" (weight: 0.004)\n",
      "- \"disease\" (weight: 0.004)\n",
      "- \"human\" (weight: 0.003)\n",
      "- \"scientist\" (weight: 0.003)\n",
      "- \"university\" (weight: 0.003)\n",
      "- \"science\" (weight: 0.003)\n",
      "- \"animal\" (weight: 0.003)\n",
      "- \"research\" (weight: 0.003)\n",
      "\n",
      "Topic 3:\n",
      "- \"health\" (weight: 0.009)\n",
      "- \"school\" (weight: 0.008)\n",
      "- \"percent\" (weight: 0.007)\n",
      "- \"state\" (weight: 0.007)\n",
      "- \"company\" (weight: 0.007)\n",
      "- \"student\" (weight: 0.006)\n",
      "- \"care\" (weight: 0.006)\n",
      "- \"program\" (weight: 0.005)\n",
      "- \"child\" (weight: 0.005)\n",
      "- \"million\" (weight: 0.004)\n",
      "\n",
      "Topic 4:\n",
      "- \"trump\" (weight: 0.028)\n",
      "- \"clinton\" (weight: 0.011)\n",
      "- \"president\" (weight: 0.010)\n",
      "- \"state\" (weight: 0.008)\n",
      "- \"republican\" (weight: 0.007)\n",
      "- \"campaign\" (weight: 0.007)\n",
      "- \"election\" (weight: 0.006)\n",
      "- \"obama\" (weight: 0.005)\n",
      "- \"vote\" (weight: 0.005)\n",
      "- \"voter\" (weight: 0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic with weight\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0cc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
