{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff5651a",
   "metadata": {},
   "source": [
    "# Lab 8 - Text Analytics CISB5123\n",
    "\n",
    "## Text Clustering\n",
    "\n",
    "###### Name: Abdul Hakiim bin Ahmad Rosli - SW01081337\n",
    "\n",
    "Text clustering groups similar documents together based on their content, allowing you\n",
    "to discover patterns, trends, and insights within large collections of text data.\n",
    "Any text clustering approach involves broadly the following steps:\n",
    "- Text pre-processing: Text can be noisy, hiding information between stop words, inflexions and sparse representations. Pre-processing makes the dataset easier to work with.\n",
    "- Feature Extraction: One of the commonly used techniques to extract the features from textual data is calculating the frequency of words/tokens in the document/corpus.\n",
    "- Clustering: We can then cluster different text documents based on the features we have generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61964f11",
   "metadata": {},
   "source": [
    "### Text Clustering Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b32dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c798d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c16d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Document\n",
    "dataset = [\"I love playing football on the weekends\",\"I enjoy hiking and camping in the mountains\",\"I like to read books and watch movies\",\"I prefer playing video games over sports\",\"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f196c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d052b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8235c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camping\n",
      " enjoy\n",
      " hiking\n",
      " mountain\n",
      " weekend\n",
      " listening\n",
      " concert\n",
      " football\n",
      " game\n",
      " going\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " playing\n",
      " football\n",
      " weekend\n",
      " going\n",
      " sport\n",
      " music\n",
      " concert\n",
      " video\n",
      " game\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulhakiim/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Perform clustering\n",
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06cf8bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baa7d8",
   "metadata": {},
   "source": [
    "### Text Clustering using Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dda6025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6afc4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdulhakiim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80d63685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the documents\n",
    "dataset = [\"I love playing football on the weekends\",\"I enjoy hiking and camping in the mountains\",\"I like to read books and watch movies\",\"I prefer playing video games over sports\",\"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ade80ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "100b11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=preprocessed_dataset, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f05e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0) for doc in preprocessed_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a3b0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulhakiim/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a1f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae49f00",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea002c12",
   "metadata": {},
   "source": [
    "#### 1. Modify the codes for both TF-IDF & Word2Vec vectorizer by adding text preprocessing steps. Do the Purity differ when applying text preprocessing before vectorization?\n",
    "\n",
    "**Answer:** \n",
    "It appears that applying text preprocessing before vectorization has different effects on the clustering results for TF-IDF and Word2Vec vectorizers.\n",
    "\n",
    "TF-IDF Vectorizer:\n",
    "- Before applying text preprocessing: Purity = 0.6\n",
    "- After applying text preprocessing: Purity = 0.8\n",
    "\n",
    "In the case of the TF-IDF vectorizer, the purity value increased from 0.6 to 0.8 after applying text preprocessing. This suggests that text preprocessing had a positive impact on the clustering results when using TF-IDF.\n",
    "Text preprocessing steps such as tokenization, lowercasing, removing stopwords, and lemmatization help in reducing noise and normalizing the text data. By removing irrelevant words (stopwords) and transforming words to their base or dictionary form (lemmatization), the preprocessed text focuses more on the meaningful content.\n",
    "The improvement in purity indicates that after preprocessing, the clusters formed by the TF-IDF vectorizer are more homogeneous and contain documents that are more similar to each other within each cluster. The preprocessing steps likely helped in better capturing the important features and improving the clustering quality.\n",
    "\n",
    "Word2Vec Vectorizer:\n",
    "- Before applying text preprocessing: Purity = 0.6\n",
    "- After applying text preprocessing: Purity = 0.6\n",
    "\n",
    "In the case of the Word2Vec vectorizer, the purity value remained the same at 0.6 before and after applying text preprocessing. This suggests that text preprocessing did not have a significant impact on the clustering results when using Word2Vec.\n",
    "\n",
    "Word2Vec is a neural network-based model that learns dense vector representations of words, capturing semantic relationships between them. The model takes into account the context of words and learns to represent words with similar meanings closer together in the vector space.\n",
    "\n",
    "The fact that the purity value did not change with preprocessing in the Word2Vec case could be due to a few reasons:\n",
    "1. Word2Vec is already capable of handling some level of noise and variations in the text data, as it learns from the context of words.\n",
    "2. The preprocessing steps applied may not have significantly altered the semantic relationships captured by Word2Vec.\n",
    "3. The specific preprocessing techniques used (e.g., removing stopwords, lemmatization) may not have had a substantial impact on the clustering results in this particular case.\n",
    "\n",
    "It's important to note that the impact of text preprocessing on clustering results can vary depending on the dataset, the specific preprocessing techniques used, and the characteristics of the vectorizer and clustering algorithm.\n",
    "\n",
    "In summary, based on the provided purity values, applying text preprocessing before vectorization had a positive impact on the clustering results for the TF-IDF vectorizer, increasing the purity from 0.6 to 0.8. However, for the Word2Vec vectorizer, text preprocessing did not lead to a change in purity, suggesting that the preprocessing steps did not significantly affect the clustering quality in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b038d8",
   "metadata": {},
   "source": [
    "#### 2. Perform text clustering on 'customer_complaints_1.csv' dataset, specifically the Text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f05dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baf5729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_complaints_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8334955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any leading/trailing whitespace\n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Remove any non-alphanumeric characters\n",
    "df['text'] = df['text'].str.replace(r'[^a-zA-Z0-9\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7c13e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer and transform the text data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfc5ef6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulhakiim/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=3, random_state=42)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1fedc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the DataFrame\n",
    "df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebb86767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " - boxes\n",
      " - second\n",
      " - adding\n",
      " - malfunction\n",
      " - investigating\n",
      " - protocol\n",
      " - floor\n",
      " - customer\n",
      " - possible\n",
      " - account\n",
      "\n",
      "Cluster 1:\n",
      " - internet\n",
      " - contract\n",
      " - comcast\n",
      " - service\n",
      " - xfinity\n",
      " - speed\n",
      " - mbps\n",
      " - customer\n",
      " - months\n",
      " - told\n",
      "\n",
      "Cluster 2:\n",
      " - rude\n",
      " - service\n",
      " - day\n",
      " - rep\n",
      " - joke\n",
      " - local\n",
      " - people\n",
      " - ignorant\n",
      " - tom\n",
      " - helpful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(3):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" - {terms[ind]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d18757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " - i've had the worst experiences so far since install on 10/4/16. nothing but problems. two no shows on scheduled service appointments, extreme difficulty in adding boxes to the second floor. what is so difficult about adding boxes to an existing account? no thank you, i'm not starting a second account for the second floor of the same house! a separate bundle package? all i wanted was just to add a few boxes. apparently this is not possible. well then, i guess it's not possible to remain a customer!\n",
      " - there is a malfunction on the dvr manager which is preventing us from adding more recordings. customer service is fairly certain that the problem is from the signal from their system to ours, but protocol demands that they access our home before investigating that option. since we work, that cannot be done until next saturday. customer service tech agreed that this seems illogical since logic would dictate that one would investigate the most probably malfunction first, but insists they must follow protocol. this is extremely frustrating. after 35 years as a customer of comcast & their predecessors, i am investigating alternatives.\n",
      "\n",
      "Cluster 1:\n",
      " - i used to love comcast. until all these constant updates. my internet and cable crash a lot at night, and sometimes during the day, some channels don't even work and on demand sometimes don't play either. i wish they will do something about it. because just a few mins ago, the internet have crashed for about 20 mins for no reason. i'm tired of it and thinking about switching to wow or something. please do not get xfinity.\n",
      " - i'm so over comcast! the worst internet provider. i'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. definitely switching to verizon. i'd rather pay $10 extra then dealing w/ comcast and non stopping internet problems.\n",
      " - if i could give them a negative star or no stars on this review i would. i have never worked with any industry with as bad of customer service as comcast. it is not a matter of money because i make well enough above and beyond to afford their services but they are a legitimate ripoff. i think they are the biggest scam of since the mortgage industry's major meltdown and i hope i move somewhere where comcast does not exist. the disregard to want to help or do the right thing is honestly astounding. if you have to call, which you do for all issues - billing, connection/service, adding or removing service, errors, it does not matter you will be transferred minimum of 4 times. everyone says the same thing and passes the issues to the next person and no one resolves the problem.they offer promotional packages in small timeframes and can never access them again so they then upgrade you without you wishing and change your billing. it has been 5 months and i have been overcharged $40 a month since i started with them. the blatant rudeness that must make you qualified to do this job is the type of quality service that gets you this review. so... dear comcast, you suck. sincerely, a customer who cannot wait to never use your service again.\n",
      " - check your contract when you sign up for comcast as their advertised offers do not match the contract they issue. i signed up for $49.99 150mbps internet for 2 years, however my contract has $19.99 for 25mbps internet for 2 years. they say there is an add on in place for $30 which boost it to blast! pro, however this isn't part of the contract, which means that comcast can increase the price whenever they want within the 2 years. this means i haven't received the advertised rate. comcast has so far refused to issue corrected contract, or issue in writing that the $30 will remain at that price for 2 years. i just have to trust them. so watch out, comcast is doing the usual illegal practices, i'm guessing to catch people out and hope they don't notice and end up paying more than they should.\n",
      " - thank god. i am changing to dish. they gave me awesome pricing and super people to deal with. you can actually understand what they are saying. i'm so excited to finally be able to return this equipment although still haven't received the home security yet as promised 4 times. go to h*ll comcast. you have made me miserable and cause me to miss many hours of work with your promises.\n",
      "\n",
      "Cluster 2:\n",
      " - charges overwhelming. comcast service rep was so ignorant and rude when i call to resolve my issue with my bill. i emailed tom ** his rep was rude to me. none of the representative was helpful. they all just pass me on to other people. i am cutting my service with comcast.\n",
      " - unplanned, unexpected, all day outages, rude service reps. rearranging my day to set up service only to deal with a totally inept technician who has to call for backup on another day. i could go on, but why? we are dropping this failure of a company and relying on hotspots from now on.\n",
      " - be warned. you will have 10$ hidden fees when you sign up for this service. they charge you extra for the local news stations and a fee for local sports that you will not be told about until you see it on your first bill. the customer service people are a joke and rude rude rude. if you ask for a manager they will get snotty and never let you talk to one... i know they have attractive packages to get you hooked but it's a joke and you will pay more in the long run than the competitors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    cluster_complaints = df[df['Cluster'] == i]['text'].values\n",
    "    for complaint in cluster_complaints[:5]:\n",
    "        print(f\" - {complaint}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f7ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
