{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bf6ca8",
   "metadata": {},
   "source": [
    "# Lab Assignment 2 - Text Analytics CISB5123\n",
    "\n",
    "Sentiment Analysis is the process of classifying the content of documents as positive, negative\n",
    "and/or neutral. In this assignment, we will explore sentiment classification using the Amazon\n",
    "Fine Food Review dataset.\n",
    "\n",
    "#### Team members:\n",
    "1. Abdul Hakiim bin Ahmad Rosli (SW01081337)\n",
    "2. Muhammad Bazly bin Burhan (SW01081224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac225b0c",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab61dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22590618",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Preview the head of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60334d7",
   "metadata": {},
   "source": [
    "#### Removing HTML tags and Unwanted characters\n",
    "It is important to clean the text data by removing the HTML tags and any unwanted characters before proceeding with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06349d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  i have bought several of the vitality canned d...  \n",
       "1      Not as Advertised  product arrived labeled as jumbo salted peanut...  \n",
       "2  \"Delight\" says it all  this is a confection that has been around a fe...  \n",
       "3         Cough Medicine  if you are looking for the secret ingredient i...  \n",
       "4            Great taffy  great taffy at a great price there was a wide ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply the clean_text function to the 'Text' column\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned text data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39e0f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               26\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413e6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for sentiment analysis\n",
    "df = df[['Score', 'Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0b81c",
   "metadata": {},
   "source": [
    "#### Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf61746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "df['Tokens'] = df['Text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd08edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the WordNet lemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "df['Tokens'] = df['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfd66fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>[i, have, bought, several, of, the, vitality, ...</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>[product, arrived, labeled, a, jumbo, salted, ...</td>\n",
       "      <td>product arrived labeled a jumbo salted peanuts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>[this, is, a, confection, that, ha, been, arou...</td>\n",
       "      <td>this is a confection that ha been around a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>[if, you, are, looking, for, the, secret, ingr...</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "      <td>[great, taffy, at, a, great, price, there, wa,...</td>\n",
       "      <td>great taffy at a great price there wa a wide a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  \\\n",
       "0      5  i have bought several of the vitality canned d...   \n",
       "1      1  product arrived labeled as jumbo salted peanut...   \n",
       "2      4  this is a confection that has been around a fe...   \n",
       "3      2  if you are looking for the secret ingredient i...   \n",
       "4      5  great taffy at a great price there was a wide ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, have, bought, several, of, the, vitality, ...   \n",
       "1  [product, arrived, labeled, a, jumbo, salted, ...   \n",
       "2  [this, is, a, confection, that, ha, been, arou...   \n",
       "3  [if, you, are, looking, for, the, secret, ingr...   \n",
       "4  [great, taffy, at, a, great, price, there, wa,...   \n",
       "\n",
       "                                   Preprocessed_Text  \n",
       "0  i have bought several of the vitality canned d...  \n",
       "1  product arrived labeled a jumbo salted peanuts...  \n",
       "2  this is a confection that ha been around a few...  \n",
       "3  if you are looking for the secret ingredient i...  \n",
       "4  great taffy at a great price there wa a wide a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the tokens back into sentences\n",
    "df['Preprocessed_Text'] = df['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "df.to_csv('preprocessed_amazon_reviews.csv', index=False)\n",
    "\n",
    "# Preview the preprocessed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6d0d2",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Let us go through the feature extraction step using the Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66732305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616644c",
   "metadata": {},
   "source": [
    "#### Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5619c16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW feature shape: (568454, 298598)\n",
      "Vocabulary size: 298598\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "bow_features = vectorizer.fit_transform(df['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the BoW features and the vocabulary size\n",
    "print(\"BoW feature shape:\", bow_features.shape)\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d6ace",
   "metadata": {},
   "source": [
    "#### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abc8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature shape: (568454, 298598)\n",
      "Vocabulary size: 298598\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the TF-IDF features and the vocabulary size\n",
    "print(\"TF-IDF feature shape:\", tfidf_features.shape)\n",
    "print(\"Vocabulary size:\", len(tfidf_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f20e02",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f289ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25e28c",
   "metadata": {},
   "source": [
    "#### 1. Lexicon-based approach using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a02e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-based Approach Accuracy: 0.7992308964313735\n"
     ]
    }
   ],
   "source": [
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Assign sentiment labels based on the 'Score' column\n",
    "def assign_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'Positive'\n",
    "    elif score <= 2:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(assign_sentiment)\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for each review\n",
    "df['Lexicon_Sentiment'] = df['Preprocessed_Text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Map sentiment scores to labels\n",
    "df['Lexicon_Sentiment_Label'] = df['Lexicon_Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
    "\n",
    "# Evaluate the lexicon-based approach\n",
    "lexicon_accuracy = accuracy_score(df['Sentiment'], df['Lexicon_Sentiment_Label'])\n",
    "print(\"Lexicon-based Approach Accuracy:\", lexicon_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b17cab",
   "metadata": {},
   "source": [
    "#### 2. Machine learning-based approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7643fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7877052713055563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.03      0.06     16181\n",
      "     Neutral       0.50      0.00      0.00      8485\n",
      "    Positive       0.79      1.00      0.88     89025\n",
      "\n",
      "    accuracy                           0.79    113691\n",
      "   macro avg       0.75      0.34      0.32    113691\n",
      "weighted avg       0.79      0.79      0.70    113691\n",
      "\n",
      "SVM Accuracy: 0.8920407068281571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.75      0.76     16181\n",
      "     Neutral       0.71      0.30      0.43      8485\n",
      "    Positive       0.92      0.97      0.94     89025\n",
      "\n",
      "    accuracy                           0.89    113691\n",
      "   macro avg       0.80      0.68      0.71    113691\n",
      "weighted avg       0.88      0.89      0.88    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Preprocessed_Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train and evaluate Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Train and evaluate SVM classifier\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59290e",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea709d5",
   "metadata": {},
   "source": [
    "In this discussion, we will analyze the strengths and weaknesses of the selected models for sentiment classification based on the experimental results and the characteristics of the Amazon Fine Food Reviews dataset.\n",
    "\n",
    "1. Lexicon-based Approach (VADER):\n",
    "    - Accuracy: The lexicon-based approach using VADER achieved an accuracy of 0.7992, which means it correctly classified approximately 79.92% of the reviews.\n",
    "    - Strengths:\n",
    "        -Interpretability: The lexicon-based approach is easily interpretable as it relies on predefined sentiment lexicons and rules to assign sentiment scores to words and phrases.\n",
    "        -Efficiency: Lexicon-based methods are computationally efficient and can provide quick sentiment predictions without the need for extensive training.\n",
    "    - Weaknesses:\n",
    "        - Limited coverage: The performance of lexicon-based methods depends on the comprehensiveness of the sentiment lexicon used. If the lexicon does not cover domain-specific or colloquial language, the sentiment predictions may be less accurate.\n",
    "        - Inability to capture context: Lexicon-based approaches often struggle with understanding the context and complex language patterns, such as sarcasm, irony, or negation, which can lead to misclassifications.\n",
    "\n",
    "2. Naive Bayes Classifier:\n",
    "    - Accuracy: The Naive Bayes classifier achieved an accuracy of 0.7877, correctly classifying approximately 78.77% of the reviews.\n",
    "    - Strengths:\n",
    "        - Simplicity: Naive Bayes is a simple and intuitive probabilistic model that is easy to implement and understand.\n",
    "        - Efficiency: Naive Bayes is computationally efficient and can handle large datasets with high-dimensional features.\n",
    "    - Weaknesses:\n",
    "        - Independence assumption: Naive Bayes assumes that the features (words) are conditionally independent given the sentiment class, which may not always hold true in natural language.\n",
    "        - Imbalanced performance: As seen in the classification report, Naive Bayes performs well in predicting the majority class (Positive) but struggles with the minority classes (Negative and Neutral), resulting in low recall and F1-scores for those classes.\n",
    "\n",
    "3. Support Vector Machine (SVM) Classifier:\n",
    "    - Accuracy: The SVM classifier achieved an accuracy of 0.8920, correctly classifying approximately 89.20% of the reviews, outperforming the other two models.\n",
    "    - Strengths:\n",
    "        - High accuracy: SVM demonstrates superior performance in sentiment classification, achieving the highest accuracy among the selected models.\n",
    "        - Handles high-dimensional data: SVM can effectively handle high-dimensional feature spaces, making it suitable for text classification tasks.\n",
    "        - Balanced performance: The classification report shows that SVM maintains good precision, recall, and F1-scores across all sentiment classes, indicating a more balanced performance compared to Naive Bayes.\n",
    "    - Weaknesses:\n",
    "        - Interpretability: SVM models are often considered as black-box models, making it challenging to interpret and explain the reasoning behind their predictions.\n",
    "        - Computational complexity: Training an SVM model can be computationally expensive, especially with large datasets and complex kernel functions.\n",
    "\n",
    "Based on the experimental results, the SVM classifier emerges as the best-performing model for sentiment classification on the Amazon Fine Food Reviews dataset. It achieves the highest accuracy and demonstrates a more balanced performance across all sentiment classes compared to the lexicon-based approach and Naive Bayes classifier.\n",
    "\n",
    "However, it is important to note that the choice of the best model depends on various factors such as the specific requirements of the application, the trade-off between accuracy and interpretability, and the computational resources available.\n",
    "\n",
    "The lexicon-based approach, despite its lower accuracy, offers the advantage of interpretability and efficiency, making it suitable for scenarios where quick sentiment predictions are needed, and the underlying reasoning is important.\n",
    "\n",
    "On the other hand, if the primary goal is to achieve high accuracy and handle complex language patterns, the SVM classifier would be the preferred choice, especially if computational resources are not a constraint.\n",
    "\n",
    "It is also worth mentioning that the performance of these models can be further improved by experimenting with different feature extraction techniques, hyperparameter tuning, and ensemble methods.\n",
    "\n",
    "Additionally, the dataset's characteristics should be considered. The Amazon Fine Food Reviews dataset contains a large number of reviews, with an imbalance in the sentiment class distribution. Addressing class imbalance through techniques like oversampling, undersampling, or class weighting can potentially enhance the models' performance, particularly for the minority classes.\n",
    "\n",
    "In conclusion, the SVM classifier demonstrates the best performance for sentiment classification on the given dataset, offering high accuracy and balanced performance across sentiment classes. However, the choice of the most suitable model depends on the specific requirements and constraints of the application, and further improvements can be explored through advanced techniques and dataset-specific considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e50b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
